{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebadf07d",
   "metadata": {},
   "source": [
    "Dataset 불러오기 -- pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d821d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\".. /파일경로\", names = [\"열1\", \"열2\", ... \"열8\"])\n",
    "\n",
    "print(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6149e9a1",
   "metadata": {},
   "source": [
    "Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b397cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[\"열 이름1\"]\n",
    "y = data[\"열 이름\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae66427",
   "metadata": {},
   "source": [
    "### SHAP modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063e2d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model \n",
    "model = xgb.XGBRegressor(objective = \"reg:squarederror\")\n",
    "model.fit(x,y)\n",
    "\n",
    "# Get predictions\n",
    "y_pred = model.predict(x)\n",
    "\n",
    "#Model evaluation\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "#prediction\n",
    "plt.scatter(y,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc44bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#비교용 y=x 그래프\n",
    "plt.plot([0, 30], [0,30], color = 'r', linestyle='-', linewidth=2)\n",
    "\n",
    "#축\n",
    "plt.y_label('Predicted', size = 20)\n",
    "plt.xlabel('Actual', size=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234b5446",
   "metadata": {},
   "source": [
    "explainaer 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4605ae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TreeExplainer는 XGBoost, LightGBM, RandomForest 같은 트리 기반 모델에 최적화된 설명 모델입니다.\n",
    "explainer = shap.TreeExplainer(model)\n",
    "# explainer(X_test)를 호출하여 SHAP 값, 데이터, 기본값 등이 모두 포함된 Explanation 객체를 생성합니다.\n",
    "explanation = explainer(X_test)\n",
    "\n",
    "\n",
    "\n",
    "np.shape(shap_values.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cd6f00",
   "metadata": {},
   "source": [
    "Plot하기 -- waterfall plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c4c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# waterfall plot for first observation\n",
    "shap.plots.waterfall(shap_values[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968c9527",
   "metadata": {},
   "source": [
    "force plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a404c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.force(shap_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9384ef9",
   "metadata": {},
   "source": [
    "stacked force plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3cf4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.force(shap_values[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c012d9",
   "metadata": {},
   "source": [
    "SHAP 이용 CO2RR 공정 분석 시작"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0272f9c3",
   "metadata": {},
   "source": [
    "step1. 라이브러리 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82d4744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from kan import KAN\n",
    "from kan.utils import ex_round\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53415b45",
   "metadata": {},
   "source": [
    "step2. 엑셀 파일 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52ec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엑셀 데이터 로드 \n",
    "filepath = r\"C:\\Users\\kepco201\\Desktop\\SHAP_Project\\CO2RR_SHAP_0807.ipynb\"\n",
    "excel_file = pd.ExcelFile(filepath) \n",
    "df_in  = pd.read_excel(excel_file, sheet_name='Input')\n",
    "df_out = pd.read_excel(excel_file, sheet_name='Output')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e363e18",
   "metadata": {},
   "source": [
    "step3.Outlier 제거 / 전체 데이터의 10% 이내로 극소량만 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6ddd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상치(Outlier) 제거 (IQR 방식) ---\n",
    "print(f\"이상치 제거 전 데이터 수: {len(df_in)} 개\")   # len(df_in) 을 통해 이상치 제거 전 데이터 길이(개수) 출력\n",
    "\n",
    "\n",
    "def remove_outliers_iqr(df_in, df_out):              # outlier 제거 함수 정의\n",
    "    \n",
    "    combined_df = pd.concat([df_in, df_out], axis=1) # pd.concat = 2개를 합치기 // 입력 변수(X)와 출력 변수(y)를 합쳐서 전체 데이터프레임 생성 // x랑 y를 한번에 고려\n",
    "                                                     # axis = 1 --- 오른쪽으로 합치기 --- 데이터 구조를 보면 예쁘게 정리가 된다\n",
    "    \n",
    "    numeric_cols = combined_df.select_dtypes(include=np.number).columns  # .select_dtypes 를 통해 특정 열만 출력 outlier 를 탐지할 숫자형 컬럼만 선택\n",
    "                                    # numeric_cols = 숫자로만 구성된 열들의 이름 리스트 (.colums = 그 열의 이름을 리스트로 출력)\n",
    "\n",
    "    # 각 컬럼에 대해 이상치 경계 계산 \n",
    "    Q1 = combined_df[numeric_cols].quantile(0.25)   # .quantile(0.25) = 데이터를 오름차순으로 정렬했을 떄 하위 25% 지점 \n",
    "    Q3 = combined_df[numeric_cols].quantile(0.75)   # .quantile(0.75) = 데이터를 오름차순으로 정렬했을 떄 상위 25% 지점 \n",
    "    IQR = Q3 - Q1 # IQR은 대략 상위 25% - 상위75% = 중간정도의 값에 해당\n",
    "    \n",
    "    lower_bound = Q1 - 6 * IQR  # 보통은 1.5* IQR을 진행하지만 최대한 삭제되는 데이터가 적도록 진행\n",
    "    upper_bound = Q3 + 6 * IQR\n",
    "    \n",
    "\n",
    "    # 밑의 줄은 공부를 더 해보자\n",
    "    # 모든 컬럼에 대해 정상 범위 내에 있는 데이터만 True로 표시\n",
    "    # (row의 어떤 컬럼이라도 이상치면 해당 row 전체가 False가 됨)\n",
    "    condition = ~((combined_df[numeric_cols] < lower_bound) | (combined_df[numeric_cols] > upper_bound)).any(axis=1)\n",
    "    \n",
    "\n",
    "    # 정상 범위에 있는 데이터만 필터링\n",
    "    df_in_no_outliers = df_in[condition]\n",
    "    df_out_no_outliers = df_out[condition]\n",
    "    \n",
    "    return df_in_no_outliers, df_out_no_outliers\n",
    "\n",
    "\n",
    "\n",
    "# 3. 함수를 사용하여 이상치 제거\n",
    "#    이전에 결측치를 제거한 df_in_cleaned, df_out_cleaned를 사용합니다.\n",
    "df_in_final, df_out_final = remove_outliers_iqr(df_in, df_out)\n",
    "\n",
    "\n",
    "# 4. 이상치 제거 후 남은 데이터 개수 확인\n",
    "removed_count = len(df_in) - len(df_in_final)  # 몇 개 지웠는지 세기\n",
    "print(f\"이상치 제거 후 데이터 수: {len(df_in_final)} 개 ({removed_count} 개 제거됨)\")\n",
    "print(\"이상치 제거 완료 \")\n",
    "\n",
    "\n",
    "# 이제 'df_in_final'과 'df_out_final'을 사용하자\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464b4e8e",
   "metadata": {},
   "source": [
    "step4. outlier 제거 후 X와 y 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbafb129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 변수(X) 선택\n",
    "# 모델이 '총 필요 에너지'를 예측하는 데 사용할 정보(컬럼)들을 선택\n",
    "X = df_in_final[[\n",
    "    \"Current density (mA/cm2)\", \n",
    "    \"Faradaic efficiency (%)\", \n",
    "    \"CO coversion\",\n",
    "    \"Voltage (V)\", \n",
    "    \"Electricity cost ($/kWh)\", \n",
    "    \"Membrain cost ($/m2)\",\n",
    "    \"Catpure energy (GJ/ton)\", \n",
    "    \"Crossover rate\"\n",
    "]].values\n",
    "\n",
    "\n",
    "\n",
    "predicting = \"Required energy_total (MJ/kgCO)\" # 다른 output 변수 보고싶으면 이거 보면 됨 # Required energy_total (MJ/kgCO) # MSP ($/kgCO)\n",
    "###### 이거를 수정해서 다른 output도 보자\n",
    "\n",
    "y = df_out_final[predicting].values.reshape(-1, 1)   # df_out_final[] 는 pandas datatframe이기에 이것을 skitlearn 이나 Keras 형태로 바꾸기 -- 그래야 \n",
    "                                                     # .values 를 통해 Numpy 배열로 변환 (값만 뽑기 때문) # 그 후에 reshape(-1.1) 을 통해 \n",
    "                                                     # .reshape()을 이용해 열 1개\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(X)\n",
    "print(\"====================\")\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a6af53",
   "metadata": {},
   "source": [
    "step5. train_set, valadation_set, test_set 만들기 (64:16:20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d373e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1단계: 먼저 train+val과 test로 분할 (80:20)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# 2단계: train+val을 train과 val로 분할 (64:16, 전체 대비)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)  # 0.2 × 0.8 = 0.16 (전체의 16%)\n",
    "\n",
    "\n",
    "# 최종 비율 확인\n",
    "# (X[:,0])~(X[:,7]) 에 각각의 입력변수들의 값들이 각각 저장됨\n",
    "\n",
    "\n",
    "\n",
    "print(X_val)\n",
    "print(f\"전체 데이터셋 크기: {len(X)}\")\n",
    "print(f\"훈련셋 크기: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"검증셋 크기: {len(X_val)} ({len(X_val)/len(X)*100:.1f}%)\")  \n",
    "print(f\"테스트셋 크기: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5ec1ba",
   "metadata": {},
   "source": [
    "step6. 데이터 전처리(0.1~0.9로 scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8188252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중요: 훈련 데이터(X_train, y_train)의 최소/최대값을 기준으로 스케일러를 학습(fit)하고,\n",
    "# 이 기준으로 모든 데이터셋(train, val, test)을 동일하게 변환합니다.\n",
    "# 이렇게 해야 테스트 과정에서 미래 정보(테스트셋의 최소/최대값)가 모델에 유출되는 것을 막을 수 있다.\n",
    "# validation dataset이나 test data로 스케일링을 할 시 데이터 누수 발생 가능\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# 1. MinMaxScaler 객체 생성 --- 범위를 0.1~0.9로 재설정\n",
    "scaler_X = MinMaxScaler(feature_range=(0.1, 0.9))\n",
    "scaler_y = MinMaxScaler(feature_range=(0.1, 0.9))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train_norm = scaler_X.fit_transform(X_train) # 훈련 데이터로 스케일러 학습 및 변환 (fit_transform)\n",
    "y_train_norm = scaler_y.fit_transform(y_train) # X_train의 각 변수(컬럼)별로 최소값은 0, 최대값은 1이 되도록 변환됩니다.\n",
    "\n",
    "# 3. 학습된 스케일러로 검증 및 테스트 데이터 변환 (transform)\n",
    "# X_train의 기준으로 나머지 데이터들을 변환합니다.\n",
    "X_val_norm = scaler_X.transform(X_val)\n",
    "X_test_norm = scaler_X.transform(X_test)\n",
    "\n",
    "y_val_norm = scaler_y.transform(y_val)   # y_val 과 y_test 도 y_train 의 정규분포를 따라 변환된다\n",
    "y_test_norm = scaler_y.transform(y_test)\n",
    "\n",
    "\n",
    "#print(X_train_norm)\n",
    "\n",
    "#print(X_val_norm)\n",
    "\n",
    "#print(X_test_norm)\n",
    "\n",
    "#print(y_train_norm)\n",
    "\n",
    "#print(y_val_norm)\n",
    "\n",
    "#print(y_test_norm)\n",
    "\n",
    "# X_train_norm 은 [[x0~x7], [x0~x7],....,[x0~x7]] 에서 각 x0~x7은 각 열마다 각각 범위가 0~1로 범위가 변환됨\n",
    "# 이 변환된 정도를 X_val_norm 과 X_test_norm도 적용받음\n",
    "\n",
    "# 정규화 후 통계 확인\n",
    "print(\"정규화 후 통계:\")\n",
    "print(f\"X_train_norm: mean={X_train_norm.mean():.4f}, std={X_train_norm.std():.4f}\")\n",
    "print(f\"X_val_norm: mean={X_val_norm.mean():.4f}, std={X_val_norm.std():.4f}\")\n",
    "print(f\"X_test_norm: mean={X_test_norm.mean():.4f}, std={X_test_norm.std():.4f}\")\n",
    "print(f\"y_train_norm: mean={y_train_norm.mean():.4f}, std={y_train_norm.std():.4f}\")\n",
    "print(f\"y_val_norm: mean={y_val_norm.mean():.4f}, std={y_val_norm.std():.4f}\")\n",
    "print(f\"y_test_norm: mean={y_test_norm.mean():.4f}, std={y_test_norm.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4074a7f8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03260b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad01fb87",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a1f470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2132406b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453db055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18c0c631",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d745d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6c5f133",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3906473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
